{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"Aula14 - Regressão Múltipla.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"az9YGuSZrB4s","colab_type":"code","colab":{}},"source":["from collections import Counter\n","from functools import partial\n","from modules.gradient_descent import minimize_stochastic\n","from modules.simple_linear_regression import total_sum_of_squares\n","import math, random\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from scipy.stats import t\n","from scipy.stats import norm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDEE4zFJrB48","colab_type":"text"},"source":["# Regressão Múltipla\n","\n","Embora a diretora da empresa esteja bastante impressionada com seu modelo preditivo, ela acha que você pode fazer melhor. Para isso, você coletou dados adicionais: para cada um dos seus usuários, você sabe quantas horas ele trabalha todos os dias e se ele tem um PhD. Você gostaria de usar esses dados adicionais para melhorar seu modelo.\n","\n","Assim, você hipotetiza um modelo linear com mais variáveis independentes:\n","\n","minutos = $\\alpha + \\beta_1$ friends + $\\beta_2$ horas de trabalho + $\\beta_3$ phd + $\\epsilon$\n","\n","Obviamente, se um usuário tem um PhD não é um número, mas - como mencionamos na Aula 10 - podemos introduzir uma variável dummy que é igual a 1 para usuários com PhDs e 0 para usuários sem. Depois disso, a variável é tão numérica quanto as outras.\n","\n","# O modelo\n","\n","Lembre-se de que, na Aula 13, ajustamos um modelo da seguinte forma:\n","\n","$$y_i = \\alpha + \\beta x_i + \\epsilon_i$$\n","\n","Agora imagine que cada entrada $x_i$ não é um simples número, mas um vetor de $k$ números $x_{i1}, \\cdots, x_{ik}$. O modelo de regressão múltipla assume que:\n","\n","$$y_i = \\alpha + \\beta_{i1} x_{i1} + \\cdots + \\beta_{ik} x_{ik} + \\epsilon_i$$\n","\n","Na regressão múltipla, o vetor de parâmetros é geralmente chamado de $\\beta$. Também queremos que isso inclua o termo constante, o que podemos conseguir adicionando uma coluna de dados aos nossos dados:\n","\n","`beta = [alpha, beta_1, ..., beta_k]`\n","\n","e:\n","\n","`x_i = [1, x_i1, ..., x_ik]`\n","\n","Então, o nosso modelo é simplesmente:"]},{"cell_type":"code","metadata":{"id":"_kSt08l_rB4-","colab_type":"code","colab":{}},"source":["def predict(x_i, beta):\n","\n","    \"\"\"assumes that the first element of each x_i is 1\"\"\"\n","    return np.dot(x_i, beta)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wa3zbZ4-rB5K","colab_type":"text"},"source":["Nesse caso particular, nossa variável independente $x$ será uma lista de vetores, cada qual se parecendo com:\n","\n","```\n","[1,  # constant term\n"," 49, # number of friends\n"," 4,  # work hours per day\n"," 0]  # doesn't have PhD\n","```\n","\n","Esses dados estão convenientemente salvos [aqui](https://www.dropbox.com/s/ixa4gtd0592jd24/multiple_dailyminutes.csv?dl=0)."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7sTw6LdzrB5M","colab_type":"code","colab":{},"outputId":"b3d3b490-caf1-471e-9227-3d782d493b16"},"source":["df = pd.read_csv('./data/multiple_dailyminutes.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>constant_term</th>\n","      <th>daily_minutes</th>\n","      <th>num_friends</th>\n","      <th>phd</th>\n","      <th>work_hours</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>68.77</td>\n","      <td>49</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>51.25</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>52.08</td>\n","      <td>40</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>38.36</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>44.54</td>\n","      <td>21</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>57.13</td>\n","      <td>21</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>51.40</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>41.42</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>31.22</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>34.76</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>54.01</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>38.79</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>47.59</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>49.10</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>27.66</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>41.03</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>36.73</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>48.65</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1</td>\n","      <td>28.12</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1</td>\n","      <td>46.62</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1</td>\n","      <td>35.57</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1</td>\n","      <td>32.98</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1</td>\n","      <td>35.00</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1</td>\n","      <td>26.07</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1</td>\n","      <td>23.77</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1</td>\n","      <td>39.73</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>1</td>\n","      <td>40.57</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1</td>\n","      <td>31.65</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>1</td>\n","      <td>31.21</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1</td>\n","      <td>36.32</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>173</th>\n","      <td>1</td>\n","      <td>19.92</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>1</td>\n","      <td>31.02</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>175</th>\n","      <td>1</td>\n","      <td>16.49</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>176</th>\n","      <td>1</td>\n","      <td>12.16</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>1</td>\n","      <td>30.70</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>178</th>\n","      <td>1</td>\n","      <td>31.22</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>179</th>\n","      <td>1</td>\n","      <td>34.65</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>180</th>\n","      <td>1</td>\n","      <td>13.13</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>181</th>\n","      <td>1</td>\n","      <td>27.51</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>182</th>\n","      <td>1</td>\n","      <td>33.20</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>183</th>\n","      <td>1</td>\n","      <td>31.57</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>184</th>\n","      <td>1</td>\n","      <td>14.10</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>185</th>\n","      <td>1</td>\n","      <td>33.42</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>186</th>\n","      <td>1</td>\n","      <td>17.44</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>187</th>\n","      <td>1</td>\n","      <td>10.12</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>188</th>\n","      <td>1</td>\n","      <td>24.42</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>189</th>\n","      <td>1</td>\n","      <td>9.82</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>190</th>\n","      <td>1</td>\n","      <td>23.39</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>191</th>\n","      <td>1</td>\n","      <td>30.93</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>192</th>\n","      <td>1</td>\n","      <td>15.03</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>193</th>\n","      <td>1</td>\n","      <td>21.67</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>194</th>\n","      <td>1</td>\n","      <td>31.09</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>1</td>\n","      <td>33.29</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>1</td>\n","      <td>22.61</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>1</td>\n","      <td>26.89</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>1</td>\n","      <td>23.48</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>1</td>\n","      <td>8.38</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>200</th>\n","      <td>1</td>\n","      <td>27.81</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>201</th>\n","      <td>1</td>\n","      <td>32.35</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>202</th>\n","      <td>1</td>\n","      <td>23.84</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>203 rows × 5 columns</p>\n","</div>"],"text/plain":["     constant_term  daily_minutes  num_friends  phd  work_hours\n","0                1          68.77           49    0           4\n","1                1          51.25           41    0           9\n","2                1          52.08           40    0           8\n","3                1          38.36           25    0           6\n","4                1          44.54           21    0           1\n","5                1          57.13           21    0           0\n","6                1          51.40           19    0           3\n","7                1          41.42           19    0           0\n","8                1          31.22           18    0           9\n","9                1          34.76           18    0           8\n","10               1          54.01           16    0           4\n","11               1          38.79           15    0           3\n","12               1          47.59           15    0           0\n","13               1          49.10           15    0           2\n","14               1          27.66           15    0           7\n","15               1          41.03           14    0           0\n","16               1          36.73           14    0           1\n","17               1          48.65           13    0           1\n","18               1          28.12           13    0           7\n","19               1          46.62           13    0           4\n","20               1          35.57           13    0           2\n","21               1          32.98           12    0           5\n","22               1          35.00           12    0           0\n","23               1          26.07           11    0           9\n","24               1          23.77           10    0           9\n","25               1          39.73           10    0           1\n","26               1          40.57           10    0           1\n","27               1          31.65           10    0           7\n","28               1          31.21           10    0           9\n","29               1          36.32           10    0           1\n","..             ...            ...          ...  ...         ...\n","173              1          19.92            2    1           5\n","174              1          31.02            2    1           0\n","175              1          16.49            2    1           9\n","176              1          12.16            2    1           9\n","177              1          30.70            2    1           0\n","178              1          31.22            2    1           1\n","179              1          34.65            2    1           1\n","180              1          13.13            2    1           4\n","181              1          27.51            1    1           0\n","182              1          33.20            1    1           2\n","183              1          31.57            1    1           2\n","184              1          14.10            1    1           5\n","185              1          33.42            1    1           3\n","186              1          17.44            1    1          10\n","187              1          10.12            1    1           6\n","188              1          24.42            1    1           0\n","189              1           9.82            1    1           8\n","190              1          23.39            1    1           6\n","191              1          30.93            1    1           4\n","192              1          15.03            1    1           9\n","193              1          21.67            1    1           9\n","194              1          31.09            1    1           4\n","195              1          33.29            1    1           2\n","196              1          22.61            1    1           9\n","197              1          26.89            1    1           0\n","198              1          23.48            1    1           8\n","199              1           8.38            1    1           6\n","200              1          27.81            1    1           1\n","201              1          32.35            1    1           1\n","202              1          23.84            1    1           5\n","\n","[203 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"I4QM1LJdrB5c","colab_type":"code","colab":{}},"source":["x = df[['constant_term','num_friends','work_hours','phd']].values\n","daily_minutes_good = df['daily_minutes'].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KoDw2zy9rB5o","colab_type":"text"},"source":["## Suposições Adicionais do Modelo dos Mínimos Quadrados\n","\n","Há algumas outras suposições que são necessárias para que esse modelo (e nossa solução) façam sentido.\n","\n","A primeira é que as colunas de $x$ são linearmente independentes, ou seja, que não há como escrever qualquer uma delas como uma soma ponderada de algumas das outras. Se essa suposição falhar, é impossível estimar o $\\beta$. Para ver isso em um caso extremo, imagine que temos um campo extra `num_acquaintances` em nossos dados que para cada usuário era exatamente igual a `num_friends`.\n","\n","Então, começando com qualquer $\\beta$, se adicionarmos *qualquer* valor ao coeficiente de `num_friends` e subtrairmos o mesmo valor do coeficiente de `num_acquaintances`, as previsões do modelo permanecerão inalteradas. O que significa que não há como encontrar o coeficiente para `num_friends`. (Normalmente, as violações dessa suposição não serão tão óbvias.)\n","\n","A segunda suposição importante é que as colunas de $x$ são todas não correlacionadas com os erros $\\epsilon$. Se isso não acontecer, nossas estimativas de $\\beta$ serão sistematicamente erradas.\n","\n","Por exemplo, na Aula 13 construímos um modelo que previa que cada amigo adicional estava associado a um acréscimo de 0,90 minutos diários no site.\n","\n","Imagine que também é o caso que:\n","\n","* Pessoas que trabalham mais horas gastam menos tempo no site.\n","\n","* Pessoas com mais amigos tendem a trabalhar mais horas.\n","\n","Ou seja, imagine que o modelo \"real\" seja:\n","\n","minutos = $\\alpha$ + $\\beta_1$ amigos + $\\beta_2$ horas de trabalho + $\\epsilon$\n","\n","e que horas de trabalho e amigos estão positivamente correlacionados. Nesse caso, quando minimizamos os erros do modelo de variável única:\n","\n","minutos = $\\alpha$ + $\\beta_1$ amigos + $\\epsilon$\n","\n","vamos subestimar $\\beta_1$.\n","\n","Pense no que aconteceria se fizéssemos previsões usando o modelo de variável única com o valor \"real\" de $\\beta_1$. (Ou seja, o valor que resulta da minimização dos erros do que chamamos de modelo \"real\".) As previsões tendem a ser grandes demais para usuários que trabalham muitas horas e pequenas demais para usuários que trabalham poucas horas, porque $\\beta_2 < 0$ e nós \"esquecemos\" de incluí-lo. Como as horas de trabalho estão correlacionadas positivamente com o número de amigos, isso significa que as previsões tendem a ser grandes demais para usuários com muitos amigos e pequenas demais para usuários com poucos amigos. Ou seja, o erro $\\epsilon$ da previsão está correlacionado com a variável independente \"amigos\".\n","\n","O resultado disso é que podemos reduzir os erros (no modelo de variável única) aumentando nossa estimativa de $\\beta_1$, o que significa que o $\\beta_1$ minimizador de erros é menor que o valor \"real\". Ou seja, neste caso, a solução de mínimos quadrados de variável única é tendenciosa para subestimar $\\beta_1$. E, em geral, sempre que as variáveis independentes estiverem correlacionadas com erros como este, nossa solução de mínimos quadrados nos dará uma estimativa tendenciosa de $\\beta$.\n","\n","Se você está com dificuldades para entender isso, imagine o que acontece quando ajustamos os dois modelos aos seguintes dados:\n","\n","|minutos|amigos|horas de trabalho|\n","|--------------------------------|\n","|200|50|10|\n","|195|52|11|\n","|150|40|5|\n","\n","Note que as colunas \"amigos\" e \"horas de trabalho\" são correlacionadas positivamente, que \"amigos\" está correlacionado positivamente com \"minutos\" ($\\beta_1 > 0$) e \"horas de trabalho\", negativamente ($\\beta_2 < 0$)."]},{"cell_type":"markdown","metadata":{"id":"vYJPEIsErB5r","colab_type":"text"},"source":["## Ajustando o modelo\n","\n","Como fizemos no modelo linear simples, escolheremos `beta` para minimizar a soma dos erros quadrados. Encontrar uma solução exata não é simples de fazer à mão, o que significa que precisaremos usar o gradiente descendente. Começaremos criando uma função de erro para minimizar. Para o gradiente descendente estocástico, queremos apenas que o erro ao quadrado corresponda a uma única previsão:"]},{"cell_type":"code","metadata":{"id":"99p5XIKorB5t","colab_type":"code","colab":{}},"source":["def error(x_i, y_i, beta):\n","    return y_i - predict(x_i, beta)\n","\n","def squared_error(x_i, y_i, beta):\n","    return error(x_i, y_i, beta) ** 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3r3dD0crrB53","colab_type":"text"},"source":["Se você sabe cálculo, você pode calcular:"]},{"cell_type":"code","metadata":{"id":"KfRgWIzLrB56","colab_type":"code","colab":{}},"source":["def squared_error_gradient(x_i, y_i, beta):\n","    \"\"\"the gradient corresponding to the ith squared error term\"\"\"\n","    return [-2 * x_ij * error(x_i, y_i, beta)\n","            for x_ij in x_i]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVJO6xHerB6F","colab_type":"text"},"source":["Caso contrário, você precisa confiar em minha palavra.\n","\n","Neste momento, estamos prontos para encontrar o `beta` ideal usando a gradiente descendente estocástico:"]},{"cell_type":"code","metadata":{"id":"n5jXvKJZrB6H","colab_type":"code","colab":{}},"source":["def estimate_beta(x, y):\n","    beta_initial = [random.random() for x_i in x[0]]\n","    return minimize_stochastic(squared_error,\n","                               squared_error_gradient,\n","                               x, y,\n","                               beta_initial,\n","                               0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EEO-hIurB6S","colab_type":"code","colab":{},"outputId":"e3f6e038-dafc-49ed-ec4a-753feb975c8d"},"source":["random.seed(0)\n","beta = estimate_beta(x, daily_minutes_good)\n","print(beta)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[30.6198817   0.97020565 -1.86719139  0.91637116]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L7RnxkgirB6e","colab_type":"text"},"source":["Isso significa que o nosso modelo é:\n","\n","minutos = $30.62$ + $0.97$ friends - $1.87$ horas de trabalho + $0.92$ phd"]},{"cell_type":"markdown","metadata":{"id":"yeEy-pMErB6h","colab_type":"text"},"source":["# Interpretando o modelo\n","\n","Você deve pensar nos coeficientes do modelo como representando estimativas para os impactos de cada fator quando todos-os-demais-são-iguais. Todos os demais sendo iguais, cada amigo adicional\n","corresponde a um minuto (ou $0.97$ minutos) extra gasto no site todos os dias. Sendo o restante igual, cada hora adicional na jornada de trabalho de um usuário corresponde a cerca de dois minutos (ou $1.87$ minutos) a menos gastos no site todos os dias. Tudo o mais sendo igual, ter um PhD está associado a gastar um minuto (ou $0.92$ minutos) extra no site todos os dias.\n","\n","O que isso não nos diz (diretamente) é nada sobre as interações entre as variáveis. É possível que o efeito das horas de trabalho seja diferente para pessoas com muitos amigos do que para pessoas com poucos amigos. Este modelo não captura isso. Uma maneira de lidar com esse caso é introduzir uma nova variável que é o produto de \"amigos\" e \"horas de trabalho\". Isso efetivamente permite que o coeficiente de \"horas de trabalho\" aumente (ou diminua) à medida que o número de amigos aumenta.\n","\n","Ou é possível que quanto mais amigos você tiver, mais tempo você gasta no site até certo ponto, após o qual mais amigos farão com que você gaste menos tempo no site. (Talvez com muitos amigos a experiência seja simplesmente demais para lidar?) Poderíamos tentar capturar isso em nosso modelo adicionando outra variável que é o *quadrado* do número de amigos.\n","\n","Uma vez que começamos a adicionar variáveis, precisamos nos preocupar se os seus coeficientes \"são importantes\". Não há limites para o número de produtos, logs, quadrados e poderes superiores que poderíamos adicionar.\n","\n","## Qualidade de ajuste\n","\n","Mais uma vez podemos olhar para o R-quadrado, que agora aumentou para 0,68:"]},{"cell_type":"code","metadata":{"id":"kxl5iWO9rB6j","colab_type":"code","colab":{}},"source":["def multiple_r_squared(x, y, beta):\n","    sum_of_squared_errors = sum(error(x_i, y_i, beta) ** 2\n","                                for x_i, y_i in zip(x, y))\n","    return 1.0 - sum_of_squared_errors / total_sum_of_squares(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqJ1rNx7rB6w","colab_type":"code","colab":{},"outputId":"a6f36f55-dad9-4346-ecaa-be5ae002112e"},"source":["print(\"R2 = \", multiple_r_squared(x, daily_minutes_good, beta))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["R2 =  0.6800074955952596\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7OoyE74GrB69","colab_type":"text"},"source":["Tenha em mente, entretanto, que adicionar novas variáveis a uma regressão *necessariamente* aumentará o R-quadrado. Afinal, o modelo de regressão simples é apenas o caso especial do modelo de regressão múltipla, onde os coeficientes em \"horas de trabalho\" e \"PhD\" são iguais a $0$. O modelo de regressão múltipla ideal terá necessariamente um erro pelo menos tão pequeno quanto o erro da regressão simples.\n","\n","Por causa disso, em uma regressão múltipla, também precisamos examinar os erros padrão dos coeficientes, que medem quão certos estamos sobre nossas estimativas de cada $\\beta_i$. A regressão como um todo pode se encaixar muito bem em nossos dados, mas se algumas das variáveis independentes são correlacionadas (ou irrelevantes), seus coeficientes podem não *significar* muito.\n","\n","A abordagem típica para medir esses erros começa com outra suposição - que os erros $\\epsilon_i$ são variáveis aleatórias normais independentes com média $0$ e algum desvio padrão compartilhado (desconhecido) $\\sigma$. Nesse caso, nós (ou, mais provavelmente, nosso software estatístico) podemos usar alguma álgebra linear para encontrar o erro padrão de cada coeficiente. Quanto maior, menor a certeza o nosso modelo tem sobre esse coeficiente. Infelizmente, não estamos preparados para fazer esse tipo de álgebra linear do zero.\n","\n","# Digressão: O Bootstrap\n","\n","Imagine que tenhamos uma amostra de $n$ pontos de dados gerados por alguma distribuição (desconhecida para nós):\n","\n","`data = get_sample(num_points = n)`\n","\n","Na Aula 4, escrevemos uma função para calcular a mediana dos dados observados, que podemos usar como uma estimativa da mediana da própria distribuição.\n","\n","Mas quão confiantes podemos ser sobre nossa estimativa? Se todos os dados da amostra estiverem muito próximos de 100, parece provável que a mediana real esteja próxima de 100. Se aproximadamente metade dos dados da amostra estiver próxima de 0 e a outra metade estiver próxima de 200, então não podemos estar tão certos quanto à mediana.\n","\n","Se pudéssemos obter repetidamente novas amostras, poderíamos calcular a mediana de cada uma e observar a distribuição dessas medianas. Normalmente não podemos. O que podemos fazer é inicializar (*to bootstrap*) novos conjuntos de dados escolhendo $n$ pontos de dados *com substituição* de nossos dados e, em seguida, calcular as medianas desses conjuntos de dados sintéticos:"]},{"cell_type":"code","metadata":{"id":"n3eIBlTHrB6_","colab_type":"code","colab":{}},"source":["def bootstrap_sample(data):\n","    \"\"\"randomly samples len(data) elements with replacement\"\"\"\n","    return [random.choice(data) for _ in data]\n","\n","def bootstrap_statistic(data, stats_fn, num_samples):\n","    \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\"\n","    return [stats_fn(bootstrap_sample(data))\n","            for _ in range(num_samples)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRQDc7elrB7I","colab_type":"text"},"source":["Por exemplo, considere os seguintes conjuntos de dados:"]},{"cell_type":"code","metadata":{"id":"Q-KdUe6-rB7K","colab_type":"code","colab":{}},"source":["# 101 points all very close to 100\n","close_to_100 = [99.5 + random.random() for _ in range(101)]\n","\n","# 101 points, 50 of them near 0, 50 of them near 200\n","far_from_100 = ([99.5 + random.random()] + \n","                [random.random() for _ in range(50)] +\n","                [200 + random.random() for _ in range(50)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tA4omtGrB7V","colab_type":"text"},"source":["Se você calcular a mediana de cada, ambos vão estar bem próximos de 100:"]},{"cell_type":"code","metadata":{"id":"fXRf8dsXrB7W","colab_type":"code","colab":{},"outputId":"b7b3861e-8b52-4bd7-f1ff-ded9486407f7"},"source":["print(\"mediana de close_to_100: \", np.median(close_to_100))\n","print(\"mediana de far_from_100: \", np.median(far_from_100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mediana de close_to_100:  100.06586584030461\n","mediana de far_from_100:  100.37472916630605\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eWz2O4MrrB7l","colab_type":"text"},"source":["No entanto, se você olhar para:"]},{"cell_type":"code","metadata":{"id":"CJPgWUkxrB7n","colab_type":"code","colab":{},"outputId":"82625967-4e23-4734-e424-2b74704bbb4c"},"source":["medians1 = bootstrap_statistic(close_to_100, np.median, 100)\n","print(\"bootstrap_statistic(close_to_100, median, 100):\")\n","fig = plt.figure()\n","plt.hist(medians1)\n","plt.xlabel(\"mediana\")\n","plt.ylabel(\"contagem\")\n","plt.title(\"medianas do conjunto close_to_100\")\n","plt.show()\n","fig.savefig('bootstrap1.png', dpi=150)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bootstrap_statistic(close_to_100, median, 100):\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<matplotlib.figure.Figure at 0x232a9613dd8>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"iaoaUJVVrB7y","colab_type":"text"},"source":["\n","você vê números, em sua maioria, muito próximos de 100. No entanto, se você observar:"]},{"cell_type":"code","metadata":{"id":"K1C00iHArB7z","colab_type":"code","colab":{},"outputId":"8e6807e5-0a8d-4ccf-b0e4-7200b8d7b3f7"},"source":["medians2 = bootstrap_statistic(far_from_100, np.median, 100)\n","print(\"bootstrap_statistic(far_from_100, median, 100):\")\n","fig = plt.figure()\n","plt.hist(medians2)\n","plt.xlabel(\"mediana\")\n","plt.ylabel(\"contagem\")\n","plt.title(\"medianas do conjunto far_from_100\")\n","plt.show()\n","fig.savefig('bootstrap2.png', dpi=150)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bootstrap_statistic(far_from_100, median, 100):\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<matplotlib.figure.Figure at 0x232a9682898>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"tJn5SZXkrB8D","colab_type":"text"},"source":["você verá muitas medianas próximas de $0$ e outras próximas de $200$.\n","\n","O desvio padrão do primeiro conjunto de medianas é próximo de $0$, enquanto o desvio padrão do segundo conjunto de medianas é próximo de $100$. (Esse caso extremo seria bem fácil de se descobrir, inspecionando manualmente os dados, mas em geral isso não é factível.)"]},{"cell_type":"markdown","metadata":{"id":"3B0IgpcorB8F","colab_type":"text"},"source":["# Erros Padrão de Coeficientes de Regressão\n","\n","Podemos usar a mesma abordagem para estimar os erros padrão de nossos coeficientes de regressão. Nós repetidamente tomamos um `bootstrap_sample` dos nossos dados e estimamos o `beta` com base nessa amostra. Se o coeficiente correspondente a uma das variáveis independentes (digamos `num_friends`) não variar muito entre as amostras, podemos ter certeza de que nossa estimativa é relativamente segura. Se o coeficiente variar muito entre as amostras, não podemos ficar confiantes em nossa estimativa.\n","\n","A única sutileza é que, antes da amostragem, precisamos zipar os dados `x` e `y` para garantir que os valores correspondentes das variáveis independentes e dependentes sejam amostrados juntos. Isso significa que `bootstrap_sample` retornará uma lista de pares `(x_i, y_i)`, que precisaremos remontar em um `x_sample` e um `y_sample`:"]},{"cell_type":"code","metadata":{"id":"C-jJNDNfrB8H","colab_type":"code","colab":{}},"source":["def estimate_sample_beta(sample):\n","    \"\"\"sample is a list of pairs (x_i, y_i)\"\"\"\n","    x_sample, y_sample = zip(*sample) # magic unzipping trick\n","    return estimate_beta(x_sample, y_sample)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoO1MGjLrB8Q","colab_type":"code","colab":{}},"source":["random.seed(0) # so that you get the same results as me\n","\n","bootstrap_betas = bootstrap_statistic(list(zip(x, daily_minutes_good)),\n","                                      estimate_sample_beta,\n","                                      100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fU-41nxmrB8Z","colab_type":"text"},"source":["Após o qual podemos estimar o desvio padrão de cada coeficiente:"]},{"cell_type":"code","metadata":{"id":"noQnSHDyrB8a","colab_type":"code","colab":{},"outputId":"05343b64-9fed-467f-9e3c-2622e449df32"},"source":["bootstrap_standard_errors = [\n","    np.std([beta[i] for beta in bootstrap_betas])\n","    for i in range(4)]\n","\n","print(\"bootstrap standard errors:\")\n","print(\"Std do coeficiente Beta do termo constante:\", bootstrap_standard_errors[0])\n","print(\"Std do coeficiente Beta do termo num_amigos:\", bootstrap_standard_errors[1])\n","print(\"Std do coeficiente Beta do termo horas_trabalho:\", bootstrap_standard_errors[2])\n","print(\"Std do coeficiente Beta do termo phd:\", bootstrap_standard_errors[3])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bootstrap standard errors:\n","Std do coeficiente Beta do termo constante: 0.9487719642256199\n","Std do coeficiente Beta do termo num_amigos: 0.06257240793036054\n","Std do coeficiente Beta do termo horas_trabalho: 0.1166351087514055\n","Std do coeficiente Beta do termo phd: 0.8548719625771628\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T0dOdAnrrB8p","colab_type":"text"},"source":["Podemos usá-los para testar hipóteses como \"$\\beta_i$ é igual a zero?\" Sob a hipótese nula $\\beta_i = 0$ (e com nossas outras suposições sobre a distribuição de $\\epsilon_i$), a estatística:\n","\n","$$t_j = \\frac{\\hat{\\beta_j}}{\\hat{\\sigma_j}}$$,\n","\n","que é a nossa estimativa de $\\beta_j$ dividido pela nossa estimativa de seu erro padrão, segue uma *distribuição t de Student* com \"n-k graus de liberdade\".\n","\n","No módulo `t` da biblioteca `scipy.stats` há todas as funções associadas a distribuição t de Student. Assim, podemos usar a CDF dessa distribuição para calcular *p-valores* para cada coeficiente de mínimos quadrados a fim de indicar a probabilidade de observarmos esse valor se o coeficiente real fosse zero. "]},{"cell_type":"code","metadata":{"id":"lxrcpLN9rB8r","colab_type":"code","colab":{}},"source":["def p_value_t(beta_hat_j, sigma_hat_j, df): #df = degrees of freedom\n","    if beta_hat_j > 0:\n","        return 2 * (1 - t.cdf(beta_hat_j / sigma_hat_j, df))\n","    else:\n","        return 2 * t.cdf(beta_hat_j / sigma_hat_j, df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"V4YPncr2rB82","colab_type":"code","colab":{},"outputId":"0183cf7e-4492-49d8-c1e9-2e746ee9a96d"},"source":["dof = 100 - 4 #n-k: amostras tem 100 valores e o modelo tem 4 preditores\n","for i in range(4):\n","    coef = beta[i]\n","    std = bootstrap_standard_errors[i]\n","    print(\"p_value(\",coef,\",\",std,\") =\", p_value_t(coef, std, dof))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["p_value( 30.619881701311712 , 0.9487719642256199 ) = 0.0\n","p_value( 0.9702056472470465 , 0.06257240793036054 ) = 0.0\n","p_value( -1.8671913880379478 , 0.1166351087514055 ) = 7.505131541677219e-29\n","p_value( 0.9163711597955347 , 0.8548719625771628 ) = 0.2864345276522129\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cU8IMyYyrB9A","colab_type":"text"},"source":["Podemos também usar a distribuição normal. À medida que os graus de liberdade aumentam, a distribuição t se aproxima mais e mais de uma normal. Em uma situação como essa, onde $n$ é muito maior que $k$, podemos usar a CDF da distribuição normal. Vamos ver se o resultado persiste:"]},{"cell_type":"code","metadata":{"id":"YF6AbSmYrB9D","colab_type":"code","colab":{}},"source":["def p_value_normal(beta_hat_j, sigma_hat_j):\n","    if beta_hat_j > 0:\n","        return 2 * (1 - norm.cdf(beta_hat_j / sigma_hat_j))\n","    else:\n","        return 2 * norm.cdf(beta_hat_j / sigma_hat_j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nI-K1_P3rB9M","colab_type":"code","colab":{},"outputId":"860f0af5-1b89-42b6-de17-0e367edfb287"},"source":["for i in range(4):\n","    coef = beta[i]\n","    std = bootstrap_standard_errors[i]\n","    print(\"p_value(\",coef,\",\",std,\") =\", p_value_normal(coef, std))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["p_value( 30.619881701311712 , 0.9487719642256199 ) = 0.0\n","p_value( 0.9702056472470465 , 0.06257240793036054 ) = 0.0\n","p_value( -1.8671913880379478 , 0.1166351087514055 ) = 1.1087885186419971e-57\n","p_value( 0.9163711597955347 , 0.8548719625771628 ) = 0.2837471357190793\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"__tY8o6arB9X","colab_type":"text"},"source":["(Em uma situação como essa, e provavemente como tudo que vimos até aqui, provavelmente estaríamos usando um software estatístico pronto que sabe bem como calcular os erros padrões exatos.)\n","\n","Apesar disso, observe que os valores encontrados pelos dois métodos foram *muito* similares. Enquanto a maioria dos coeficientes tem p-valores muito pequenos (sugerindo que eles são realmente diferentes de zero), o coeficiente para \"PhD\" não é \"significativamente\" diferente de zero, o que torna provável que o coeficiente para \"PhD\" seja aleatório ao invés de significativo.\n","\n","Em cenários de regressão mais elaborados, você às vezes deseja testar hipóteses mais elaboradas sobre os dados, como \"pelo menos um dos $\\beta_j$ é diferente de zero\" ou \"$\\beta_1$ é igual a $\\beta_2$ e $\\beta_3$ é igual a $\\beta_4$. Isso você pode fazer com um F-test, que, infelizmente, está fora do escopo deste curso, mas [é fácil de você aprender fora daqui](http://reliawiki.org/index.php/Multiple_Linear_Regression_Analysis)."]},{"cell_type":"markdown","metadata":{"id":"gfaeCke-rB9Y","colab_type":"text"},"source":["## Regularização\n","\n","Na prática, você costuma aplicar a regressão linear a conjuntos de dados com um grande número de variáveis. Isso cria algumas rugas extras. Primeiro, quanto mais variáveis você usar, maior a probabilidade de você sobreajustar (*overfit*) o seu modelo ao conjunto de treinamento. Em segundo lugar, quanto mais coeficientes diferentes de zero você tiver, mais difícil será entendê-los. Se o objetivo é *explicar* alguns fenômenos, um modelo esparso com três fatores pode ser mais útil do que um modelo um pouco melhor com centenas.\n","\n","A regularização é uma abordagem na qual adicionamos ao termo de erro uma penalidade que aumenta à medida que o tamanho de $\\beta$ aumenta. Em seguida, minimizamos o erro e a penalidade combinados. Quanto mais importância atribuímos ao termo de penalidade, mais desencorajamos grandes coeficientes.\n","\n","Por exemplo, na regressão de crista (*ridge regression*), adicionamos uma penalidade proporcional à soma dos quadrados do `beta_i`. (Exceção feita ao `beta_0`, o termo constante, que normalmente não é penalizado.)"]},{"cell_type":"code","metadata":{"id":"ZtsL2snGrB9a","colab_type":"code","colab":{}},"source":["# alpha is a *hyperparameter* controlling how harsh the penalty is\n","# sometimes it's called \"lambda\" but that already means something in Python\n","def ridge_penalty(beta, alpha):\n","    return alpha * np.dot(beta[1:], beta[1:])\n","\n","def squared_error_ridge(x_i, y_i, beta, alpha):\n","    \"\"\"estimate error plus ridge penalty on beta\"\"\"\n","    return error(x_i, y_i, beta) ** 2 + ridge_penalty(beta, alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8AmMIsirB9m","colab_type":"text"},"source":["Que você pode plugar no gradiente descendente da maneira usual:"]},{"cell_type":"code","metadata":{"id":"f1-gk9djrB9o","colab_type":"code","colab":{}},"source":["def ridge_penalty_gradient(beta, alpha):\n","    \"\"\"gradient of just the ridge penalty\"\"\"\n","    return [0] + [2 * alpha * beta_j for beta_j in beta[1:]]\n","\n","def squared_error_ridge_gradient(x_i, y_i, beta, alpha):\n","    \"\"\"the gradient corresponding to the ith squared error term\n","    including the ridge penalty\"\"\"\n","    regular_gradient = squared_error_gradient(x_i, y_i, beta)\n","    ridge_gradient = ridge_penalty_gradient(beta, alpha)\n","    return np.add(regular_gradient, ridge_gradient)\n","    #return vector_add(squared_error_gradient(x_i, y_i, beta),\n","    #                  ridge_penalty_gradient(beta, alpha))\n","\n","    \n","def estimate_beta_ridge(x, y, alpha):\n","    \"\"\"use gradient descent to fit a ridge regression\n","    with penalty alpha\"\"\"\n","    beta_initial = [random.random() for x_i in x[0]]\n","    return minimize_stochastic(partial(squared_error_ridge, alpha=alpha),\n","                               partial(squared_error_ridge_gradient,\n","                                       alpha=alpha),\n","                               x, y,\n","                               beta_initial,\n","                               0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojKNZsynrB9x","colab_type":"text"},"source":["Com o alfa definido como zero, não há penalidade e obtemos os mesmos resultados de antes. Mas à medida que aumentamos o valor de alfa, o ajuste vai ficando pior, e o tamanho do beta menor."]},{"cell_type":"code","metadata":{"id":"7xW6nGIRrB91","colab_type":"code","colab":{},"outputId":"f27f6b0c-a59a-421f-b858-e0123f420a65"},"source":["random.seed(0)\n","for alpha in [0.0, 0.01, 0.1, 1, 10]:\n","    beta = estimate_beta_ridge(x, daily_minutes_good, alpha=alpha)\n","    print(\"alpha:\", alpha)\n","    print(\"beta:\", beta)\n","    print(\"magnitude of beta:\", np.linalg.norm(beta[1:]))\n","    print(\"r-squared:\", multiple_r_squared(x, daily_minutes_good, beta))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["alpha: 0.0\n","beta: [30.6198817   0.97020565 -1.86719139  0.91637116]\n","magnitude of beta: 2.2950901463816518\n","r-squared: 0.6800074955952596\n","\n","alpha: 0.01\n","beta: [30.55985205  0.97306554 -1.86244246  0.93176656]\n","magnitude of beta: 2.298638157131645\n","r-squared: 0.6800102132970789\n","\n","alpha: 0.1\n","beta: [30.89486018  0.94902752 -1.85017209  0.53251297]\n","magnitude of beta: 2.1464761971841027\n","r-squared: 0.6797276241305292\n","\n","alpha: 1\n","beta: [30.66677891  0.908636   -1.6938673   0.09370161]\n","magnitude of beta: 1.9244702679726162\n","r-squared: 0.6757061537631814\n","\n","alpha: 10\n","beta: [ 2.83728611e+01  7.30766086e-01 -9.21216318e-01 -1.84955517e-02]\n","magnitude of beta: 1.1760104858585188\n","r-squared: 0.5752138470466857\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xz8hduyUrB9_","colab_type":"text"},"source":["Em particular, o coeficiente de \"PhD\" desaparece à medida que aumentamos a penalidade, o que está de acordo com nosso resultado anterior de que o coeficiente dessa variável não é significativamente diferente de zero.\n","\n","Normalmente, você deseja redimensionar seus dados antes de usar essa abordagem. Afinal, se você mudou \"anos de experiência\" para \"séculos de experiência\", seu coeficiente de mínimos quadrados aumentaria em um fator de 100 e, de repente, seria penalizado muito mais, mesmo que fosse o mesmo modelo.\n","\n","Outra abordagem é a regressão *lasso*, que usa a penalidade:"]},{"cell_type":"code","metadata":{"id":"Wg_ZalDDrB-B","colab_type":"code","colab":{}},"source":["def lasso_penalty(beta, alpha):\n","    return alpha * sum(abs(beta_i) for beta_i in beta[1:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PuDlSxO1rB-b","colab_type":"text"},"source":["Enquanto a penalidade *ridge* encolhe os coeficientes como um todo, a penalidade do laço tende a forçar os coeficientes a serem zero, o que a torna boa para aprender modelos esparsos. Infelizmente, essa abordagem não é passível de ser otimizada a partir do gradiente descendente, o que significa que não poderemos resolvê-la do zero."]},{"cell_type":"markdown","metadata":{"id":"dvB-5L2OrB-e","colab_type":"text"},"source":["## Para explorar\n","\n","* Regressão tem uma teoria rica e expansiva por trás. Este é outro conteúdo sobre qual você deve considerar ler um livro ou, pelo menos, muitos artigos da Wikipédia.\n","\n","* O `scikit-learn` possui [um módulo de modelos lineares](http://scikit-learn.org/stable/modules/linear_model.html) que fornece um modelo `LinearRegression` semelhante ao nosso, assim como de regressão *Ridge*, *Lasso* e outros tipos de regularização também.\n","\n","* [Statsmodels](http://www.statsmodels.org/stable/index.html) é outro módulo do Python que contém (entre outras coisas) modelos de regressão linear."]}]}